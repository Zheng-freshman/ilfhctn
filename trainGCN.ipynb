{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ProgramData\\anaconda3\\envs\\usb\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from semilearn import get_config\n",
    "from semilearn.datasets import get_fracture\n",
    "from semilearn.datasets import WeightedDistributedSampler\n",
    "from semilearn.core.utils import get_data_loader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, roc_curve, auc, classification_report\n",
    "from semilearn.nets.fusionnet import GCN, MulitDropoutGCN, load_adj\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from semilearn.core.criterions import CELoss\n",
    "\n",
    "config = {\n",
    "    'algorithm': 'fullysupervised',\n",
    "    'net': 'GCN',\n",
    "    'use_pretrain': False, \n",
    "    'pretrain_path': 'https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth',\n",
    "\n",
    "    # optimization configs\n",
    "    'epoch': 100,  # set to 100\n",
    "    'num_train_iter': 1500,  # set to 102400\n",
    "    #'num_eval_iter': 300,   # set to 1024\n",
    "    #'num_log_iter': 100,    # set to 256\n",
    "    'optim': 'AdamW',\n",
    "    'lr': 5e-4,\n",
    "    'layer_decay': 0.5,\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 16,\n",
    "\n",
    "\n",
    "    # dataset configs\n",
    "    'data_dir': 'E:/Project/dataset',\n",
    "    #'train_sampler': 'RandomSampler',\n",
    "\n",
    "    # algorithm specific configs\n",
    "    'hard_label': True,\n",
    "    'uratio': 1,\n",
    "    'ulb_loss_ratio': 1.0,\n",
    "\n",
    "    # device configs\n",
    "    'gpu': 0,\n",
    "    'world_size': 1,\n",
    "    'distributed': False,\n",
    "    \"num_workers\": 2,}\n",
    "args = get_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: data inited\n",
      "DATASET: data inited\n",
      "DATASET: data inited\n",
      "DATASET: data inited\n",
      "Dataset lb: 240\n",
      "Dataset ulb: 959\n",
      "unlabeled data number: 959, labeled data number 240\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'targets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m lb_dest_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_lb\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munlabeled data number: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, labeled data number \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ulb_dest_len, lb_dest_len))\n\u001b[1;32m----> 9\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mWeightedDistributedSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.51\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2.86\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_lb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_replicas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_train_iter\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m loader_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     11\u001b[0m loader_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_lb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m get_data_loader(args,\n\u001b[0;32m     12\u001b[0m                                             dataset_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_lb\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     13\u001b[0m                                             args\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m                                             num_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mepoch,\n\u001b[0;32m     17\u001b[0m                                             num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_workers)\n",
      "File \u001b[1;32me:\\Github\\Semi-supervised-learning\\semilearn\\datasets\\samplers\\sampler.py:86\u001b[0m, in \u001b[0;36mWeightedDistributedSampler.__init__\u001b[1;34m(self, weights, dataset, num_replicas, rank, num_samples, replacement)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dataset, num_replicas, rank, num_samples)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement \u001b[38;5;241m=\u001b[39m replacement\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sample_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Github\\Semi-supervised-learning\\semilearn\\datasets\\samplers\\sampler.py:89\u001b[0m, in \u001b[0;36mWeightedDistributedSampler.get_sample_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sample_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights):\n\u001b[1;32m---> 89\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\n\u001b[0;32m     90\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([weights[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets])\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample_weight\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'targets'"
     ]
    }
   ],
   "source": [
    "lb_dset, ulb_dset, eval_dset = get_fracture(args=args, alg='fullysupervised', include_lb_to_ulb=True)\n",
    "test_dset = None\n",
    "dataset_dict = {'train_lb': lb_dset, 'train_ulb': ulb_dset, 'eval': eval_dset, 'test': test_dset}\n",
    "\n",
    "ulb_dest_len = len(dataset_dict['train_ulb']) if dataset_dict['train_ulb'] is not None else 0\n",
    "lb_dest_len = len(dataset_dict['train_lb'])\n",
    "print(\"unlabeled data number: {}, labeled data number {}\".format(ulb_dest_len, lb_dest_len))\n",
    "\n",
    "sample = WeightedDistributedSampler(weights=[1.51,2.86],dataset=dataset_dict['train_lb'],num_replicas=1,rank=0,num_samples=args.batch_size*args.num_train_iter//args.epoch)\n",
    "loader_dict = {}\n",
    "loader_dict['train_lb'] = get_data_loader(args,\n",
    "                                            dataset_dict['train_lb'],\n",
    "                                            args.batch_size,\n",
    "                                            data_sampler=sample, #args.train_sampler,\n",
    "                                            num_iters=args.num_train_iter,\n",
    "                                            num_epochs=args.epoch,\n",
    "                                            num_workers=args.num_workers)\n",
    "\n",
    "loader_dict['eval'] = get_data_loader(args,\n",
    "                                        dataset_dict['eval'],\n",
    "                                        args.eval_batch_size,\n",
    "                                        # make sure data_sampler is None for evaluation\n",
    "                                        data_sampler=None,\n",
    "                                        num_workers=args.num_workers,\n",
    "                                        drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,loader_dict,adj):\n",
    "    model.eval()\n",
    "    eval_loader = loader_dict['eval']\n",
    "    total_loss = 0.0\n",
    "    total_num = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "    y_logits = []\n",
    "    with torch.no_grad():\n",
    "        for data in eval_loader:\n",
    "            x = data['x_lb']\n",
    "            y = data['y_lb']\n",
    "            \n",
    "            if isinstance(x, dict):\n",
    "                x = {k: v.cuda(0) for k, v in x.items()}\n",
    "            else:\n",
    "                x = x.cuda(0)\n",
    "            y = y.cuda(0)\n",
    "\n",
    "            num_batch = y.shape[0]\n",
    "            total_num += num_batch\n",
    "\n",
    "            logits = model(x=data['x_lb_t'].to(\"cuda:0\"), adj=adj.to(\"cuda:0\"))\n",
    "            #print(logits)\n",
    "            #print(torch.mean(logits, dim=1))\n",
    "            pred = torch.nn.functional.softmax(torch.mean(logits, dim=1), dim=1)\n",
    "\n",
    "            loss = F.cross_entropy(pred, y, reduction='mean', ignore_index=-1)\n",
    "            \n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(torch.max(pred, dim=-1)[1].cpu().tolist())\n",
    "            y_logits.append(logits.cpu().numpy())\n",
    "            y_probs.extend(torch.softmax(logits, dim=-1).cpu().tolist())\n",
    "            total_loss += loss.item() * num_batch\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_logits = np.concatenate(y_logits)\n",
    "    #print(y_true, y_pred)\n",
    "    top1 = accuracy_score(y_true, y_pred)\n",
    "    print('acc:',top1)\n",
    "    balanced_top1 = balanced_accuracy_score(y_true, y_pred)\n",
    "    print('balanced_acc:',balanced_top1)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    print('AUC:',auc_value)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print('classification_report:\\n',report)\n",
    "\n",
    "    cf_mat = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    print('confusion matrix:\\n' + np.array_str(cf_mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNLoss, self).__init__()\n",
    "        self.ce_loss = self.ce_loss = CELoss()\n",
    "    def forward(self, logits, targets): #gt=0\n",
    "        preds = torch.mean(logits, dim=1)\n",
    "        preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "        return self.ce_loss(preds,targets,reduction='mean')\n",
    "\n",
    "net = GCN(nfeat=1,nclass=2,nhid=[2,4],dropout=0.2)\n",
    "net.train()\n",
    "opt = optim.Adam(net.parameters(), lr=args.lr)\n",
    "adj = load_adj(path=\"adj.cites\", node_num=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(\"cuda:0\")\n",
    "evaluate(net, loader_dict, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1\n",
    "it=1\n",
    "gcnloss = GCNLoss()\n",
    "net.to(\"cuda:0\")\n",
    "for epoch in range(0, args.epoch):\n",
    "    print(f\"epoch:{epoch}\")\n",
    "    # prevent the training iterations exceed args.num_train_iter\n",
    "    if it >= args.num_train_iter:\n",
    "        break\n",
    "\n",
    "    for data_lb in loader_dict['train_lb']:\n",
    "        # prevent the training iterations exceed args.num_train_iter\n",
    "        if it >= args.num_train_iter:\n",
    "            break\n",
    "        out = net(x=data_lb['x_lb_t'].to(\"cuda:0\"), adj=adj.to(\"cuda:0\"))\n",
    "        train_loss = gcnloss(out, data_lb['y_lb'].to(\"cuda:0\"))\n",
    "        train_loss.backward()\n",
    "        opt.step()\n",
    "        if it%args.num_eval_iter==0:\n",
    "            evaluate(net, loader_dict, adj)\n",
    "        it += 1\n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
